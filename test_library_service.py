#!/usr/bin/env python3
"""
æ–‡æ¡£åº“æœåŠ¡æµ‹è¯• - æµ‹è¯•Repositoryå±‚å’ŒServiceå±‚

ä¸“é—¨æµ‹è¯•æ–°å¼€å‘çš„æ–‡æ¡£åº“ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬Repositoryæ¨¡å¼å’Œä¸šåŠ¡é€»è¾‘ã€‚
"""

import sys
import tempfile
import sqlite3
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_document_repository():
    """æµ‹è¯•æ–‡æ¡£Repository"""
    print("ğŸ§ª æµ‹è¯•æ–‡æ¡£Repository...")

    try:
        from database.connection import DatabaseConnection
        from database.migrations import DatabaseMigrator
        from database.models import DocumentModel
        from repositories.document_repository import DocumentRepository

        # åˆå§‹åŒ–æ•°æ®åº“
        with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as temp_db:
            db_path = temp_db.name

        db = DatabaseConnection(db_path)
        migrator = DatabaseMigrator(db)
        migrator.migrate()

        # åˆ›å»ºRepository
        doc_repo = DocumentRepository(db)

        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:
            temp_pdf.write(b"test PDF content")
            pdf_path = temp_pdf.name

        # æµ‹è¯•åˆ›å»ºæ–‡æ¡£
        doc = DocumentModel.from_file(pdf_path, "test_hash", "Test Document")
        created_doc = doc_repo.create(doc)

        assert created_doc.id is not None
        assert created_doc.title == "Test Document"
        print(f"  âœ… æ–‡æ¡£åˆ›å»ºæˆåŠŸï¼ŒID: {created_doc.id}")

        # æµ‹è¯•æ ¹æ®å“ˆå¸ŒæŸ¥æ‰¾
        found_doc = doc_repo.find_by_file_hash("test_hash")
        assert found_doc is not None
        assert found_doc.id == created_doc.id
        print("  âœ… æ ¹æ®å“ˆå¸ŒæŸ¥æ‰¾æˆåŠŸ")

        # æµ‹è¯•æ ‡é¢˜æœç´¢
        search_results = doc_repo.search_by_title("Test")
        assert len(search_results) == 1
        assert search_results[0].title == "Test Document"
        print("  âœ… æ ‡é¢˜æœç´¢æˆåŠŸ")

        # æµ‹è¯•æ›´æ–°è®¿é—®æ—¶é—´
        success = doc_repo.update_access_time(created_doc.id)
        assert success is True
        print("  âœ… è®¿é—®æ—¶é—´æ›´æ–°æˆåŠŸ")

        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = doc_repo.get_statistics()
        assert stats["total_documents"] == 1
        assert "size_stats" in stats
        print(f"  ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats['total_documents']} ä¸ªæ–‡æ¡£")

        # æ¸…ç†
        Path(pdf_path).unlink(missing_ok=True)
        db.close_connection()
        Path(db_path).unlink(missing_ok=True)

        print("âœ… æ–‡æ¡£Repositoryæµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ æ–‡æ¡£Repositoryæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_vector_repository():
    """æµ‹è¯•å‘é‡ç´¢å¼•Repository"""
    print("ğŸ§ª æµ‹è¯•å‘é‡ç´¢å¼•Repository...")

    try:
        from database.connection import DatabaseConnection
        from database.migrations import DatabaseMigrator
        from database.models import DocumentModel, VectorIndexModel
        from repositories.document_repository import DocumentRepository
        from repositories.vector_repository import VectorIndexRepository

        # åˆå§‹åŒ–æ•°æ®åº“
        with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as temp_db:
            db_path = temp_db.name

        db = DatabaseConnection(db_path)
        migrator = DatabaseMigrator(db)
        migrator.migrate()

        # åˆ›å»ºRepositories
        doc_repo = DocumentRepository(db)
        vector_repo = VectorIndexRepository(db)

        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:
            temp_pdf.write(b"test PDF content")
            pdf_path = temp_pdf.name

        doc = DocumentModel.from_file(pdf_path, "test_hash", "Test Document")
        created_doc = doc_repo.create(doc)

        # åˆ›å»ºå‘é‡ç´¢å¼•
        with tempfile.TemporaryDirectory() as temp_dir:
            vector_index = VectorIndexModel(
                document_id=created_doc.id,
                index_path=temp_dir,
                index_hash="vector_hash_123",
                chunk_count=10
            )

            created_index = vector_repo.create(vector_index)
            assert created_index.id is not None
            print(f"  âœ… å‘é‡ç´¢å¼•åˆ›å»ºæˆåŠŸï¼ŒID: {created_index.id}")

            # æµ‹è¯•æ ¹æ®æ–‡æ¡£IDæŸ¥æ‰¾
            found_index = vector_repo.find_by_document_id(created_doc.id)
            assert found_index is not None
            assert found_index.document_id == created_doc.id
            print("  âœ… æ ¹æ®æ–‡æ¡£IDæŸ¥æ‰¾æˆåŠŸ")

            # æµ‹è¯•æ ¹æ®ç´¢å¼•å“ˆå¸ŒæŸ¥æ‰¾
            found_by_hash = vector_repo.find_by_index_hash("vector_hash_123")
            assert found_by_hash is not None
            assert found_by_hash.index_hash == "vector_hash_123"
            print("  âœ… æ ¹æ®ç´¢å¼•å“ˆå¸ŒæŸ¥æ‰¾æˆåŠŸ")

            # æµ‹è¯•å¸¦æ–‡æ¡£ä¿¡æ¯çš„æŸ¥æ‰¾
            with_docs = vector_repo.find_all_with_documents()
            assert len(with_docs) == 1
            assert with_docs[0]["document_title"] == "Test Document"
            print("  âœ… å¸¦æ–‡æ¡£ä¿¡æ¯æŸ¥æ‰¾æˆåŠŸ")

            # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
            stats = vector_repo.get_index_statistics()
            assert stats["total_indexes"] == 1
            print(f"  ğŸ“Š ç´¢å¼•ç»Ÿè®¡: {stats['total_indexes']} ä¸ªç´¢å¼•")

        # æ¸…ç†
        Path(pdf_path).unlink(missing_ok=True)
        db.close_connection()
        Path(db_path).unlink(missing_ok=True)

        print("âœ… å‘é‡ç´¢å¼•Repositoryæµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ å‘é‡ç´¢å¼•Repositoryæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_document_library_service():
    """æµ‹è¯•æ–‡æ¡£åº“æœåŠ¡"""
    print("ğŸ§ª æµ‹è¯•æ–‡æ¡£åº“æœåŠ¡...")

    try:
        from database.connection import DatabaseConnection
        from database.migrations import DatabaseMigrator
        # ç›´æ¥å¯¼å…¥æœåŠ¡å±‚æ¨¡å—
        sys.path.insert(0, str(Path(__file__).parent / "src" / "services"))
        from document_library_service import DocumentLibraryService, DuplicateDocumentError

        # åˆå§‹åŒ–æ•°æ®åº“
        with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as temp_db:
            db_path = temp_db.name

        db = DatabaseConnection(db_path)
        migrator = DatabaseMigrator(db)
        migrator.migrate()

        # åˆ›å»ºæœåŠ¡
        library_service = DocumentLibraryService(db)

        # åˆ›å»ºæµ‹è¯•PDFæ–‡ä»¶
        try:
            import fitz
            temp_pdf_fd, pdf_path = tempfile.mkstemp(suffix='.pdf')
            import os
            os.close(temp_pdf_fd)

            doc = fitz.open()
            page = doc.new_page()
            page.insert_text((100, 100), "Test library service content")
            doc.save(pdf_path)
            doc.close()
            print("  ğŸ“„ æµ‹è¯•PDFåˆ›å»ºå®Œæˆ")
        except ImportError:
            # å¦‚æœæ²¡æœ‰PyMuPDFï¼Œåˆ›å»ºå‡PDF
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:
                temp_pdf.write(b"fake PDF content")
                pdf_path = temp_pdf.name
            print("  ğŸ“„ æµ‹è¯•æ–‡ä»¶åˆ›å»ºå®Œæˆï¼ˆæ— PyMuPDFï¼‰")

        # æµ‹è¯•æ–‡æ¡£å¯¼å…¥
        imported_doc = library_service.import_document(pdf_path, "Test Library Document")
        assert imported_doc.id is not None
        assert imported_doc.title == "Test Library Document"
        print(f"  âœ… æ–‡æ¡£å¯¼å…¥æˆåŠŸï¼ŒID: {imported_doc.id}")

        # æµ‹è¯•é‡å¤å¯¼å…¥æ£€æµ‹
        try:
            library_service.import_document(pdf_path, "Duplicate Document")
            assert False, "åº”è¯¥æŠ›å‡ºé‡å¤æ–‡æ¡£å¼‚å¸¸"
        except DuplicateDocumentError:
            print("  âœ… é‡å¤æ–‡æ¡£æ£€æµ‹æˆåŠŸ")

        # æµ‹è¯•è¦†ç›–é‡å¤æ–‡æ¡£
        overwritten_doc = library_service.import_document(
            pdf_path,
            "Overwritten Document",
            overwrite_duplicates=True
        )
        assert overwritten_doc.id == imported_doc.id
        assert overwritten_doc.title == "Overwritten Document"
        print("  âœ… æ–‡æ¡£è¦†ç›–æˆåŠŸ")

        # æµ‹è¯•è·å–æ–‡æ¡£åˆ—è¡¨
        docs = library_service.get_documents()
        assert len(docs) == 1
        assert docs[0].title == "Overwritten Document"
        print("  âœ… è·å–æ–‡æ¡£åˆ—è¡¨æˆåŠŸ")

        # æµ‹è¯•æŒ‰IDè·å–æ–‡æ¡£
        doc_by_id = library_service.get_document_by_id(imported_doc.id)
        assert doc_by_id is not None
        assert doc_by_id.id == imported_doc.id
        print("  âœ… æŒ‰IDè·å–æ–‡æ¡£æˆåŠŸ")

        # æµ‹è¯•æœç´¢
        search_results = library_service.get_documents(search_query="Overwritten")
        assert len(search_results) == 1
        print("  âœ… æ–‡æ¡£æœç´¢æˆåŠŸ")

        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = library_service.get_library_statistics()
        assert "documents" in stats
        assert stats["documents"]["total_documents"] == 1
        print(f"  ğŸ“Š åº“ç»Ÿè®¡: {stats['documents']['total_documents']} ä¸ªæ–‡æ¡£")

        # æµ‹è¯•å®Œæ•´æ€§éªŒè¯
        integrity = library_service.verify_document_integrity(imported_doc.id)
        assert integrity["exists"] is True
        print(f"  ğŸ” å®Œæ•´æ€§æ£€æŸ¥: {'å¥åº·' if integrity['is_healthy'] else 'æœ‰é—®é¢˜'}")

        # æµ‹è¯•åˆ é™¤æ–‡æ¡£
        deleted = library_service.delete_document(imported_doc.id)
        assert deleted is True
        print("  âœ… æ–‡æ¡£åˆ é™¤æˆåŠŸ")

        # éªŒè¯åˆ é™¤
        deleted_doc = library_service.get_document_by_id(imported_doc.id)
        assert deleted_doc is None
        print("  âœ… åˆ é™¤éªŒè¯æˆåŠŸ")

        # æ¸…ç†
        Path(pdf_path).unlink(missing_ok=True)
        db.close_connection()
        Path(db_path).unlink(missing_ok=True)

        print("âœ… æ–‡æ¡£åº“æœåŠ¡æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ æ–‡æ¡£åº“æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_duplicate_detection():
    """æµ‹è¯•é‡å¤æ–‡æ¡£æ£€æµ‹åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•é‡å¤æ–‡æ¡£æ£€æµ‹...")

    try:
        from database.connection import DatabaseConnection
        from database.migrations import DatabaseMigrator
        # ç›´æ¥å¯¼å…¥æœåŠ¡å±‚æ¨¡å—
        sys.path.insert(0, str(Path(__file__).parent / "src" / "services"))
        from document_library_service import DocumentLibraryService

        # åˆå§‹åŒ–æ•°æ®åº“
        with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as temp_db:
            db_path = temp_db.name

        db = DatabaseConnection(db_path)
        migrator = DatabaseMigrator(db)
        migrator.migrate()

        library_service = DocumentLibraryService(db)

        # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ–‡ä»¶
        test_files = []

        try:
            import fitz

            # åˆ›å»ºç›¸åŒå¤§å°çš„æ–‡ä»¶
            for i in range(3):
                temp_pdf_fd, pdf_path = tempfile.mkstemp(suffix='.pdf')
                import os
                os.close(temp_pdf_fd)

                doc = fitz.open()
                page = doc.new_page()
                page.insert_text((100, 100), f"Document {i} with same size")
                doc.save(pdf_path)
                doc.close()

                test_files.append(pdf_path)

                # å¯¼å…¥æ–‡æ¡£
                library_service.import_document(pdf_path, f"Document {i}")

            print(f"  ğŸ“„ åˆ›å»ºäº† {len(test_files)} ä¸ªæµ‹è¯•æ–‡æ¡£")

        except ImportError:
            print("  âš ï¸ PyMuPDFä¸å¯ç”¨ï¼Œè·³è¿‡è¯¦ç»†é‡å¤æ£€æµ‹æµ‹è¯•")
            return True

        # æµ‹è¯•é‡å¤æ£€æµ‹
        duplicates = library_service.find_duplicate_documents()
        print(f"  ğŸ” å‘ç° {len(duplicates)} ç»„æ½œåœ¨é‡å¤æ–‡æ¡£")

        # æ˜¾ç¤ºé‡å¤ç»„
        for criteria, docs in duplicates:
            print(f"    ğŸ“Š {criteria}: {len(docs)} ä¸ªæ–‡æ¡£")
            for doc in docs:
                print(f"      - {doc.title} (ID: {doc.id})")

        # æ¸…ç†
        for pdf_path in test_files:
            Path(pdf_path).unlink(missing_ok=True)

        db.close_connection()
        Path(db_path).unlink(missing_ok=True)

        print("âœ… é‡å¤æ–‡æ¡£æ£€æµ‹æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ é‡å¤æ–‡æ¡£æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_library_cleanup():
    """æµ‹è¯•åº“æ¸…ç†åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åº“æ¸…ç†åŠŸèƒ½...")

    try:
        from database.connection import DatabaseConnection
        from database.migrations import DatabaseMigrator
        # ç›´æ¥å¯¼å…¥æœåŠ¡å±‚æ¨¡å—
        sys.path.insert(0, str(Path(__file__).parent / "src" / "services"))
        from document_library_service import DocumentLibraryService
        from repositories.vector_repository import VectorIndexRepository
        from database.models import VectorIndexModel

        # åˆå§‹åŒ–æ•°æ®åº“
        with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as temp_db:
            db_path = temp_db.name

        db = DatabaseConnection(db_path)
        migrator = DatabaseMigrator(db)
        migrator.migrate()

        library_service = DocumentLibraryService(db)
        vector_repo = VectorIndexRepository(db)

        # ä¸´æ—¶ç¦ç”¨å¤–é”®çº¦æŸæ¥åˆ›å»ºå­¤ç«‹ç´¢å¼•
        db.execute("PRAGMA foreign_keys = OFF")

        # åˆ›å»ºå­¤ç«‹çš„å‘é‡ç´¢å¼•ï¼ˆæ²¡æœ‰å¯¹åº”æ–‡æ¡£ï¼‰
        orphaned_index = VectorIndexModel(
            document_id=999,  # ä¸å­˜åœ¨çš„æ–‡æ¡£ID
            index_path="/fake/path",
            index_hash="orphaned_hash"
        )
        vector_repo.create(orphaned_index)

        # é‡æ–°å¯ç”¨å¤–é”®çº¦æŸ
        db.execute("PRAGMA foreign_keys = ON")
        print("  ğŸ—‘ï¸ åˆ›å»ºäº†å­¤ç«‹å‘é‡ç´¢å¼•")

        # æµ‹è¯•æ¸…ç†å‰ç»Ÿè®¡
        stats_before = library_service.get_library_statistics()
        orphaned_before = stats_before["vector_indexes"]["orphaned_count"]
        print(f"  ğŸ“Š æ¸…ç†å‰ï¼š{orphaned_before} ä¸ªå­¤ç«‹ç´¢å¼•")

        # æ‰§è¡Œæ¸…ç†
        cleanup_results = library_service.cleanup_library()
        print(f"  ğŸ§¹ æ¸…ç†ç»“æœ: {cleanup_results}")

        # æµ‹è¯•æ¸…ç†åç»Ÿè®¡
        stats_after = library_service.get_library_statistics()
        orphaned_after = stats_after["vector_indexes"]["orphaned_count"]
        print(f"  ğŸ“Š æ¸…ç†åï¼š{orphaned_after} ä¸ªå­¤ç«‹ç´¢å¼•")

        assert orphaned_after < orphaned_before
        print("  âœ… å­¤ç«‹ç´¢å¼•æ¸…ç†æˆåŠŸ")

        # æ¸…ç†
        db.close_connection()
        Path(db_path).unlink(missing_ok=True)

        print("âœ… åº“æ¸…ç†åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ åº“æ¸…ç†åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æ–‡æ¡£åº“æœåŠ¡æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ–‡æ¡£åº“æœåŠ¡åŠŸèƒ½...\n")

    tests = [
        test_document_repository,
        test_vector_repository,
        test_document_library_service,
        test_duplicate_detection,
        test_library_cleanup
    ]

    passed = 0
    total = len(tests)

    for test_func in tests:
        if test_func():
            passed += 1
        print()  # ç©ºè¡Œåˆ†éš”

    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ–‡æ¡£åº“æœåŠ¡æµ‹è¯•é€šè¿‡ï¼")
        print("\nğŸ“‹ ç¬¬äºŒé˜¶æ®µå®Œæˆï¼š")
        print("âœ… Repositoryå±‚ï¼ˆæ•°æ®è®¿é—®ï¼‰")
        print("âœ… Serviceå±‚ï¼ˆä¸šåŠ¡é€»è¾‘ï¼‰")
        print("âœ… é‡å¤æ£€æµ‹åŠŸèƒ½")
        print("âœ… åº“ç®¡ç†åŠŸèƒ½")
        print("\nğŸš€ ä¸‹ä¸€æ­¥ï¼šåˆ›å»ºUIç»„ä»¶å’Œæ§åˆ¶å™¨")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)