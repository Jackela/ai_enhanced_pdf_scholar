name: 🧪 Optimized Testing

on:
  workflow_call:
    inputs:
      frontend-changed:
        required: true
        type: string
      backend-changed:
        required: true
        type: string
      force-full:
        required: false
        type: string
        default: 'false'

env:
  NODE_VERSION: '22.17.0'
  PYTHON_VERSION: '3.11'

jobs:
  # 🧪 Backend Unit Tests (Fast)
  backend-unit-tests:
    name: 🧪 Backend Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 8
    if: inputs.backend-changed == 'true' || inputs.force-full == 'true'
    steps:
      - name: 📥 Checkout
        uses: actions/checkout@v4

      - name: 🐍 Setup Python with Cache
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements*.txt'

      - name: 🔧 Enhanced Test Cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            .venv
            .pytest_cache
          key: test-deps-v2-${{ runner.os }}-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            test-deps-v2-${{ runner.os }}-
            backend-deps-v2-${{ runner.os }}-

      - name: 📦 Install Test Dependencies (Fast)
        run: |
          echo "🚀 Installing test dependencies..."
          
          # Upgrade pip with timeout
          python -m pip install --upgrade pip --timeout 30
          
          # Install test requirements (faster than full requirements)
          if [ -f requirements-test.txt ]; then
            echo "📦 Installing optimized test dependencies..."
            pip install --timeout 60 -r requirements-test.txt
          else
            echo "📦 Installing basic test dependencies..."
            pip install --timeout 60 pytest pytest-cov pytest-mock fastapi pydantic
          fi
          
          # Verify test setup
          python -c "import pytest; print('✅ Test framework ready')"
        timeout-minutes: 3

      - name: 🧪 Run Unit Tests
        run: |
          echo "🧪 Running backend unit tests..."
          
          # Run tests with optimized settings
          pytest tests/unit/ -v \
            --tb=short \
            --maxfail=5 \
            --cov=src \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-fail-under=8 \
            --junit-xml=unit-test-results.xml \
            -x
        timeout-minutes: 4

      - name: 📊 Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: backend-unit-test-results
          path: |
            unit-test-results.xml
            coverage.xml
          retention-days: 7

  # 🧪 Repository Tests (Core)
  repository-tests:
    name: 🧪 Repository Tests
    runs-on: ubuntu-latest
    timeout-minutes: 6
    if: inputs.backend-changed == 'true' || inputs.force-full == 'true'
    steps:
      - name: 📥 Checkout
        uses: actions/checkout@v4

      - name: 🐍 Setup Python with Cache
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements*.txt'

      - name: 🔧 Repository Test Cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            .venv
            .pytest_cache
          key: repo-test-deps-v2-${{ runner.os }}-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            test-deps-v2-${{ runner.os }}-
            repo-test-deps-v1-${{ runner.os }}-

      - name: 📦 Install Dependencies
        run: |
          echo "🚀 Installing repository test dependencies..."
          python -m pip install --upgrade pip --timeout 30
          
          if [ -f requirements-test.txt ]; then
            pip install --timeout 60 -r requirements-test.txt
          else
            pip install --timeout 60 pytest pytest-cov fastapi pydantic
          fi
        timeout-minutes: 3

      - name: 🧪 Run Repository Tests
        run: |
          echo "🧪 Running repository tests..."
          
          pytest tests/repositories/ -v \
            --tb=short \
            --maxfail=3 \
            --cov=src.repositories \
            --cov-report=xml \
            --cov-report=term-missing \
            --junit-xml=repo-test-results.xml \
            -x
        timeout-minutes: 2

      - name: 📊 Upload Repository Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: repository-test-results
          path: |
            repo-test-results.xml
            coverage.xml
          retention-days: 7

  # 🧪 Service Tests (Core)
  service-tests:
    name: 🧪 Service Tests
    runs-on: ubuntu-latest
    timeout-minutes: 8
    if: inputs.backend-changed == 'true' || inputs.force-full == 'true'
    steps:
      - name: 📥 Checkout
        uses: actions/checkout@v4

      - name: 🐍 Setup Python with Cache
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements*.txt'

      - name: 🔧 Service Test Cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            .venv
            .pytest_cache
          key: service-test-deps-v2-${{ runner.os }}-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            test-deps-v2-${{ runner.os }}-
            service-test-deps-v1-${{ runner.os }}-

      - name: 📦 Install Dependencies
        run: |
          echo "🚀 Installing service test dependencies..."
          python -m pip install --upgrade pip --timeout 30
          
          if [ -f requirements-test.txt ]; then
            pip install --timeout 60 -r requirements-test.txt
          else
            pip install --timeout 60 pytest pytest-cov pytest-mock fastapi pydantic
          fi
        timeout-minutes: 3

      - name: 🧪 Run Service Tests
        run: |
          echo "🧪 Running service tests..."
          
          pytest tests/services/ -v \
            --tb=short \
            --maxfail=3 \
            --cov=src.services \
            --cov-report=xml \
            --cov-report=term-missing \
            --junit-xml=service-test-results.xml \
            -x
        timeout-minutes: 4

      - name: 📊 Upload Service Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: service-test-results
          path: |
            service-test-results.xml
            coverage.xml
          retention-days: 7

  # 🚀 Frontend Tests (Optimized)
  frontend-tests:
    name: 🚀 Frontend Tests
    runs-on: ubuntu-latest
    timeout-minutes: 6
    if: inputs.frontend-changed == 'true' || inputs.force-full == 'true'
    steps:
      - name: 📥 Checkout
        uses: actions/checkout@v4

      - name: 🚀 Setup Node.js with Cache
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: 🔧 Frontend Test Cache
        uses: actions/cache@v4
        with:
          path: |
            frontend/node_modules
            frontend/.vitest
          key: frontend-test-${{ runner.os }}-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            frontend-test-${{ runner.os }}-

      - name: 📦 Install Dependencies
        run: |
          cd frontend
          npm ci --prefer-offline --no-audit --no-fund --silent
        timeout-minutes: 2

      - name: 🧪 Run Frontend Tests
        run: |
          cd frontend
          npm test -- --run --reporter=verbose --coverage
        timeout-minutes: 3

      - name: 📊 Upload Frontend Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: frontend-test-results
          path: |
            frontend/coverage/
          retention-days: 7

  # 📋 Test Summary
  test-summary:
    name: 📋 Test Summary
    runs-on: ubuntu-latest
    needs: [backend-unit-tests, repository-tests, service-tests, frontend-tests]
    if: always()
    timeout-minutes: 1
    steps:
      - name: 📊 Generate Test Report
        run: |
          echo "# 🧪 Optimized Testing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## 🧪 Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status | Duration |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|---------|----------|" >> $GITHUB_STEP_SUMMARY
          echo "| 🧪 Backend Unit Tests | ${{ needs.backend-unit-tests.result }} | ~5m |" >> $GITHUB_STEP_SUMMARY
          echo "| 🧪 Repository Tests | ${{ needs.repository-tests.result }} | ~3m |" >> $GITHUB_STEP_SUMMARY
          echo "| 🧪 Service Tests | ${{ needs.service-tests.result }} | ~4m |" >> $GITHUB_STEP_SUMMARY
          echo "| 🚀 Frontend Tests | ${{ needs.frontend-tests.result }} | ~4m |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Calculate success metrics
          total_tests=4
          passed_tests=0
          
          [[ "${{ needs.backend-unit-tests.result }}" == "success" || "${{ needs.backend-unit-tests.result }}" == "skipped" ]] && ((passed_tests++))
          [[ "${{ needs.repository-tests.result }}" == "success" || "${{ needs.repository-tests.result }}" == "skipped" ]] && ((passed_tests++))
          [[ "${{ needs.service-tests.result }}" == "success" || "${{ needs.service-tests.result }}" == "skipped" ]] && ((passed_tests++))
          [[ "${{ needs.frontend-tests.result }}" == "success" || "${{ needs.frontend-tests.result }}" == "skipped" ]] && ((passed_tests++))
          
          success_rate=$((passed_tests * 100 / total_tests))
          
          echo "## 📊 Test Metrics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Success Rate**: ${success_rate}%" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Duration**: ~8 minutes" >> $GITHUB_STEP_SUMMARY
          echo "- **Optimizations**: Fast dependency installation, enhanced caching" >> $GITHUB_STEP_SUMMARY
          echo "- **Coverage**: Generated for all backend components" >> $GITHUB_STEP_SUMMARY
          
          if [ $success_rate -ge 75 ]; then
            echo "✅ **Test Gate: PASSED**" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Test Gate: FAILED**" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: 🎯 Test Gate Decision
        run: |
          # Check if core tests pass
          if [[ "${{ needs.backend-unit-tests.result }}" == "failure" ]]; then
            echo "❌ Critical backend tests failed. Blocking pipeline."
            exit 1
          else
            echo "✅ Test gate passed. Core functionality validated."
          fi