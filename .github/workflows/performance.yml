name: ⚡ Performance Testing Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run performance benchmarks daily at 1 AM UTC
    - cron: '0 1 * * *'
  workflow_dispatch:
    inputs:
      benchmark_level:
        description: 'Benchmark level'
        required: false
        default: 'standard'
        type: choice
        options: ['basic', 'standard', 'comprehensive']
      load_test_duration:
        description: 'Load test duration (minutes)'
        required: false
        default: '5'
        type: string

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '22'
  TIMEOUT_STANDARD: 15
  TIMEOUT_EXTENDED: 30
  BENCHMARK_LEVEL: ${{ github.event.inputs.benchmark_level || 'standard' }}
  LOAD_TEST_DURATION: ${{ github.event.inputs.load_test_duration || '5' }}

concurrency:
  group: performance-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # API performance benchmarking
  api-performance:
    name: 🚀 API Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    outputs:
      api-response-time: ${{ steps.api-metrics.outputs.avg-response-time }}
      throughput: ${{ steps.api-metrics.outputs.requests-per-second }}
      error-rate: ${{ steps.api-metrics.outputs.error-rate }}
    steps:
      - name: 📥 Checkout
        uses: actions/checkout@v4

      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: 🔧 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install locust pytest-benchmark httpx
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: 🚀 Start Application
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_db
        run: |
          python -m uvicorn src.main:app --host 0.0.0.0 --port 8000 &
          APP_PID=$!
          echo "APP_PID=$APP_PID" >> $GITHUB_ENV
          sleep 10  # Wait for app to start

      - name: 🔍 Health Check
        run: |
          curl -f http://localhost:8000/health || exit 1
          echo "Application health check passed"

      - name: ⚡ Basic API Benchmarks
        run: |
          python -c "
          import httpx
          import time
          import statistics
          
          client = httpx.Client(base_url='http://localhost:8000')
          
          # Warmup
          for _ in range(5):
              client.get('/health')
          
          # Benchmark health endpoint
          health_times = []
          for _ in range(100):
              start = time.time()
              response = client.get('/health')
              end = time.time()
              if response.status_code == 200:
                  health_times.append((end - start) * 1000)
          
          avg_health = statistics.mean(health_times)
          print(f'Health endpoint avg response time: {avg_health:.2f}ms')
          
          # Store metrics
          with open('benchmark-results.txt', 'w') as f:
              f.write(f'health_avg_ms={avg_health:.2f}\n')
          "

      - name: 🧪 Load Testing with Locust
        if: env.BENCHMARK_LEVEL != 'basic'
        run: |
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          
          class APIUser(HttpUser):
              wait_time = between(1, 3)
              
              @task(3)
              def health_check(self):
                  self.client.get("/health")
              
              @task(1)
              def api_endpoint(self):
                  # Test main API endpoints if available
                  self.client.get("/api/documents", catch_response=True)
          EOF
          
          # Run load test
          locust -f locustfile.py --headless -u 10 -r 2 -t ${LOAD_TEST_DURATION}m --html locust-report.html --csv locust-stats &
          LOCUST_PID=$!
          wait $LOCUST_PID || true

      - name: 📊 Analyze Performance Metrics
        id: api-metrics
        run: |
          # Process benchmark results
          if [ -f benchmark-results.txt ]; then
            source benchmark-results.txt
          else
            health_avg_ms=0
          fi
          
          # Process Locust results if available
          if [ -f locust-stats_stats.csv ]; then
            # Extract key metrics from Locust CSV
            AVG_RESPONSE=$(awk -F',' 'NR==2 {print $7}' locust-stats_stats.csv || echo "0")
            RPS=$(awk -F',' 'NR==2 {print $8}' locust-stats_stats.csv || echo "0")
            FAILURES=$(awk -F',' 'NR==2 {print $5}' locust-stats_stats.csv || echo "0")
            TOTAL_REQUESTS=$(awk -F',' 'NR==2 {print $4}' locust-stats_stats.csv || echo "1")
            ERROR_RATE=$(echo "scale=2; $FAILURES * 100 / $TOTAL_REQUESTS" | bc || echo "0")
          else
            AVG_RESPONSE=$health_avg_ms
            RPS=0
            ERROR_RATE=0
          fi
          
          echo "avg-response-time=$AVG_RESPONSE" >> $GITHUB_OUTPUT
          echo "requests-per-second=$RPS" >> $GITHUB_OUTPUT
          echo "error-rate=$ERROR_RATE" >> $GITHUB_OUTPUT
          
          echo "## ⚡ API Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Average Response Time:** ${AVG_RESPONSE}ms" >> $GITHUB_STEP_SUMMARY
          echo "- **Requests Per Second:** $RPS" >> $GITHUB_STEP_SUMMARY
          echo "- **Error Rate:** ${ERROR_RATE}%" >> $GITHUB_STEP_SUMMARY

      - name: 🛑 Stop Application
        if: always()
        run: |
          if [ ! -z "$APP_PID" ]; then
            kill $APP_PID || true
          fi

      - name: 📦 Upload Performance Reports
        uses: actions/upload-artifact@v4
        with:
          name: api-performance-reports-${{ github.sha }}
          path: |
            locust-report.html
            locust-stats*.csv
            benchmark-results.txt
          retention-days: 30

  # Database performance testing
  database-performance:
    name: 🗄️ Database Performance
    runs-on: ubuntu-latest
    timeout-minutes: 15
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    outputs:
      query-performance: ${{ steps.db-metrics.outputs.avg-query-time }}
      connection-time: ${{ steps.db-metrics.outputs.connection-time }}
    steps:
      - name: 📥 Checkout
        uses: actions/checkout@v4

      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: 🔧 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install psycopg2-binary sqlalchemy pytest-benchmark
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: 🗄️ Database Performance Tests
        id: db-metrics
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_db
        run: |
          python -c "
          import time
          import statistics
          import psycopg2
          from sqlalchemy import create_engine
          
          # Test database connection time
          connection_times = []
          for _ in range(10):
              start = time.time()
              conn = psycopg2.connect(
                  host='localhost',
                  database='test_db',
                  user='test_user',
                  password='test_password'
              )
              end = time.time()
              connection_times.append((end - start) * 1000)
              conn.close()
          
          avg_conn_time = statistics.mean(connection_times)
          
          # Test query performance
          engine = create_engine('postgresql://test_user:test_password@localhost:5432/test_db')
          conn = engine.connect()
          
          # Create test table
          conn.execute('''CREATE TABLE IF NOT EXISTS perf_test (
              id SERIAL PRIMARY KEY,
              data TEXT,
              created_at TIMESTAMP DEFAULT NOW()
          )''')
          
          # Insert test data
          for i in range(1000):
              conn.execute('INSERT INTO perf_test (data) VALUES (%s)', (f'test_data_{i}',))
          
          # Benchmark queries
          query_times = []
          for _ in range(50):
              start = time.time()
              result = conn.execute('SELECT COUNT(*) FROM perf_test').fetchone()
              end = time.time()
              query_times.append((end - start) * 1000)
          
          avg_query_time = statistics.mean(query_times)
          
          conn.close()
          
          print(f'Average connection time: {avg_conn_time:.2f}ms')
          print(f'Average query time: {avg_query_time:.2f}ms')
          
          # Output for GitHub Actions
          with open('db-metrics.txt', 'w') as f:
              f.write(f'connection-time={avg_conn_time:.2f}\n')
              f.write(f'avg-query-time={avg_query_time:.2f}\n')
          "
          
          # Set outputs
          source db-metrics.txt
          echo "connection-time=$connection_time" >> $GITHUB_OUTPUT
          echo "avg-query-time=$avg_query_time" >> $GITHUB_OUTPUT
          
          echo "## 🗄️ Database Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Average Connection Time:** ${connection_time}ms" >> $GITHUB_STEP_SUMMARY
          echo "- **Average Query Time:** ${avg_query_time}ms" >> $GITHUB_STEP_SUMMARY

  # Frontend performance testing
  frontend-performance:
    name: 🎨 Frontend Performance
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      build-size: ${{ steps.frontend-metrics.outputs.build-size }}
      lighthouse-score: ${{ steps.lighthouse.outputs.performance-score }}
    steps:
      - name: 📥 Checkout
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: 🔧 Install Dependencies
        working-directory: ./frontend
        run: npm ci

      - name: 🏗️ Build Frontend
        working-directory: ./frontend
        run: |
          npm run build
          echo "Frontend build completed"

      - name: 📏 Analyze Bundle Size
        id: frontend-metrics
        working-directory: ./frontend
        run: |
          # Calculate total build size
          BUILD_SIZE=$(du -sh dist/ | cut -f1)
          BUILD_SIZE_BYTES=$(du -sb dist/ | cut -f1)
          
          echo "build-size=$BUILD_SIZE" >> $GITHUB_OUTPUT
          echo "build-size-bytes=$BUILD_SIZE_BYTES" >> $GITHUB_OUTPUT
          
          echo "## 🎨 Frontend Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Build Size:** $BUILD_SIZE" >> $GITHUB_STEP_SUMMARY

      - name: 🚀 Serve Built Application
        working-directory: ./frontend
        run: |
          npx serve -s dist -p 3000 &
          SERVE_PID=$!
          echo "SERVE_PID=$SERVE_PID" >> $GITHUB_ENV
          sleep 5  # Wait for server to start

      - name: 💡 Lighthouse Performance Audit
        id: lighthouse
        if: env.BENCHMARK_LEVEL != 'basic'
        run: |
          npm install -g @lhci/cli lighthouse
          
          # Run Lighthouse audit
          lighthouse http://localhost:3000 --output=json --output-path=lighthouse-report.json --chrome-flags="--headless --no-sandbox --disable-dev-shm-usage" --only-categories=performance || true
          
          # Extract performance score if report exists
          if [ -f lighthouse-report.json ]; then
            PERFORMANCE_SCORE=$(jq -r '.categories.performance.score * 100' lighthouse-report.json 2>/dev/null || echo "0")
          else
            PERFORMANCE_SCORE=0
          fi
          
          echo "performance-score=$PERFORMANCE_SCORE" >> $GITHUB_OUTPUT
          echo "- **Lighthouse Performance Score:** ${PERFORMANCE_SCORE}/100" >> $GITHUB_STEP_SUMMARY

      - name: 🛑 Stop Frontend Server
        if: always()
        run: |
          if [ ! -z "$SERVE_PID" ]; then
            kill $SERVE_PID || true
          fi

      - name: 📦 Upload Frontend Performance Reports
        uses: actions/upload-artifact@v4
        with:
          name: frontend-performance-reports-${{ github.sha }}
          path: |
            lighthouse-report.json
            frontend/dist/
          retention-days: 30

  # Performance regression detection
  performance-regression:
    name: 📈 Performance Regression Check
    needs: [api-performance, database-performance, frontend-performance]
    if: always() && github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: 📊 Performance Regression Analysis
        run: |
          echo "## 📈 Performance Regression Analysis" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Current | Status | Notes |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|---------|--------|-------|" >> $GITHUB_STEP_SUMMARY
          
          # API Performance Analysis
          API_RESPONSE="${{ needs.api-performance.outputs.api-response-time }}"
          if (( $(echo "$API_RESPONSE > 1000" | bc -l 2>/dev/null || echo "0") )); then
            echo "| API Response Time | ${API_RESPONSE}ms | ⚠️ SLOW | > 1000ms |" >> $GITHUB_STEP_SUMMARY
            REGRESSION_FLAG=true
          else
            echo "| API Response Time | ${API_RESPONSE}ms | ✅ GOOD | < 1000ms |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Database Performance Analysis
          DB_QUERY="${{ needs.database-performance.outputs.query-performance }}"
          if (( $(echo "$DB_QUERY > 100" | bc -l 2>/dev/null || echo "0") )); then
            echo "| DB Query Time | ${DB_QUERY}ms | ⚠️ SLOW | > 100ms |" >> $GITHUB_STEP_SUMMARY
            REGRESSION_FLAG=true
          else
            echo "| DB Query Time | ${DB_QUERY}ms | ✅ GOOD | < 100ms |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Frontend Performance Analysis
          LIGHTHOUSE_SCORE="${{ needs.frontend-performance.outputs.lighthouse-score }}"
          if [ "$LIGHTHOUSE_SCORE" != "0" ] && (( $(echo "$LIGHTHOUSE_SCORE < 80" | bc -l 2>/dev/null || echo "0") )); then
            echo "| Lighthouse Score | ${LIGHTHOUSE_SCORE}/100 | ⚠️ POOR | < 80 |" >> $GITHUB_STEP_SUMMARY
            REGRESSION_FLAG=true
          else
            echo "| Lighthouse Score | ${LIGHTHOUSE_SCORE}/100 | ✅ GOOD | ≥ 80 |" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "$REGRESSION_FLAG" = true ]; then
            echo "## ⚠️ Performance Regression Detected" >> $GITHUB_STEP_SUMMARY
            echo "Please review the performance metrics and consider optimizations before merging." >> $GITHUB_STEP_SUMMARY
          else
            echo "## ✅ No Performance Regression Detected" >> $GITHUB_STEP_SUMMARY
            echo "All performance metrics are within acceptable thresholds." >> $GITHUB_STEP_SUMMARY
          fi