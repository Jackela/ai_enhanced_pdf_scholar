name: ðŸŽ¯ Multi-Stage Quality Gate

on:
  workflow_call:
    inputs:
      gate_level:
        description: 'Quality gate level (basic/standard/strict/enterprise)'
        required: false
        default: 'standard'
        type: string
      code_coverage_threshold:
        description: 'Minimum code coverage percentage'
        required: false
        default: '80'
        type: string
      performance_threshold:
        description: 'Performance regression threshold (%)'
        required: false
        default: '10'
        type: string
    outputs:
      quality_score:
        description: 'Overall quality score'
        value: ${{ jobs.quality-score.outputs.quality_score }}
      gate_status:
        description: 'Quality gate status'
        value: ${{ jobs.quality-score.outputs.gate_status }}
  workflow_dispatch:
    inputs:
      gate_level:
        description: 'Quality gate level'
        required: false
        default: 'standard'
        type: choice
        options: ['basic', 'standard', 'strict', 'enterprise']
      code_coverage_threshold:
        description: 'Minimum code coverage percentage'
        required: false
        default: '80'
        type: string
      performance_threshold:
        description: 'Performance regression threshold (%)'
        required: false
        default: '10'
        type: string
      skip_slow_checks:
        description: 'Skip slow quality checks for faster feedback'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '22.17.0'
  GATE_LEVEL: ${{ github.event.inputs.gate_level || 'standard' }}
  COVERAGE_THRESHOLD: ${{ github.event.inputs.code_coverage_threshold || '80' }}
  PERFORMANCE_THRESHOLD: ${{ github.event.inputs.performance_threshold || '10' }}

concurrency:
  group: quality-gate-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ðŸ” Gate 1: Code Quality Analysis
  code-quality-gate:
    name: ðŸ” Gate 1 - Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      quality_score: ${{ steps.analysis.outputs.quality_score }}
      issues_count: ${{ steps.analysis.outputs.issues_count }}
      gate_status: ${{ steps.analysis.outputs.gate_status }}
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis

      - name: ðŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: ðŸ“¦ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: ðŸ”§ Install Analysis Tools
        run: |
          # Python tools
          pip install ruff black mypy bandit radon vulture
          
          # JavaScript tools if frontend exists
          if [ -f frontend/package.json ]; then
            cd frontend
            npm ci --prefer-offline --no-audit
            npm install eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin
            cd ..
          fi

      - name: ðŸ” Python Code Quality Analysis
        id: python-analysis
        run: |
          echo "ðŸ” Analyzing Python code quality..."
          
          # Initialize counters
          TOTAL_ISSUES=0
          QUALITY_SCORE=100
          
          # Ruff linting
          echo "Running Ruff linting..."
          RUFF_ISSUES=$(ruff check src backend --output-format=json | jq '. | length' || echo 0)
          echo "Ruff issues: $RUFF_ISSUES"
          
          # Black formatting
          echo "Checking Black formatting..."
          BLACK_ISSUES=0
          if ! black --check src backend; then
            BLACK_ISSUES=1
          fi
          echo "Black formatting issues: $BLACK_ISSUES"
          
          # MyPy type checking
          echo "Running MyPy type checking..."
          MYPY_ISSUES=0
          if ! mypy src backend --ignore-missing-imports; then
            MYPY_ISSUES=1
          fi
          echo "MyPy type issues: $MYPY_ISSUES"
          
          # Bandit security analysis
          echo "Running Bandit security analysis..."
          BANDIT_ISSUES=$(bandit -r src backend -f json | jq '.results | length' || echo 0)
          echo "Bandit security issues: $BANDIT_ISSUES"
          
          # Code complexity analysis
          echo "Analyzing code complexity..."
          COMPLEXITY_SCORE=$(radon cc src backend -a -j | jq '[.[] | .[] | select(.type == "function") | .complexity] | add / length' || echo 0)
          echo "Average complexity: $COMPLEXITY_SCORE"
          
          # Dead code detection
          echo "Detecting dead code..."
          DEAD_CODE_ISSUES=$(vulture src backend --json | jq '. | length' || echo 0)
          echo "Dead code issues: $DEAD_CODE_ISSUES"
          
          # Calculate Python quality score
          PYTHON_TOTAL_ISSUES=$((RUFF_ISSUES + BLACK_ISSUES + MYPY_ISSUES + BANDIT_ISSUES + DEAD_CODE_ISSUES))
          PYTHON_QUALITY_SCORE=$((100 - (PYTHON_TOTAL_ISSUES * 2)))
          
          # Ensure minimum score
          if [ $PYTHON_QUALITY_SCORE -lt 0 ]; then
            PYTHON_QUALITY_SCORE=0
          fi
          
          echo "Python quality score: $PYTHON_QUALITY_SCORE"
          echo "python_issues=$PYTHON_TOTAL_ISSUES" >> $GITHUB_OUTPUT
          echo "python_quality_score=$PYTHON_QUALITY_SCORE" >> $GITHUB_OUTPUT

      - name: ðŸ” JavaScript Code Quality Analysis
        id: js-analysis
        if: hashFiles('frontend/package.json') != ''
        working-directory: frontend
        run: |
          echo "ðŸ” Analyzing JavaScript/TypeScript code quality..."
          
          # ESLint analysis
          ESLINT_ISSUES=$(npx eslint src/ --format json | jq '. | map(.messages | length) | add' || echo 0)
          echo "ESLint issues: $ESLINT_ISSUES"
          
          # TypeScript compilation check
          TS_ISSUES=0
          if ! npx tsc --noEmit; then
            TS_ISSUES=1
          fi
          echo "TypeScript compilation issues: $TS_ISSUES"
          
          # Calculate JavaScript quality score
          JS_TOTAL_ISSUES=$((ESLINT_ISSUES + TS_ISSUES))
          JS_QUALITY_SCORE=$((100 - (JS_TOTAL_ISSUES * 3)))
          
          if [ $JS_QUALITY_SCORE -lt 0 ]; then
            JS_QUALITY_SCORE=0
          fi
          
          echo "JavaScript quality score: $JS_QUALITY_SCORE"
          echo "js_issues=$JS_TOTAL_ISSUES" >> $GITHUB_OUTPUT
          echo "js_quality_score=$JS_QUALITY_SCORE" >> $GITHUB_OUTPUT

      - name: ðŸ“Š Code Quality Gate Analysis
        id: analysis
        run: |
          echo "ðŸ“Š Analyzing overall code quality gate..."
          
          # Get results from previous steps
          PYTHON_ISSUES=${{ steps.python-analysis.outputs.python_issues || 0 }}
          PYTHON_SCORE=${{ steps.python-analysis.outputs.python_quality_score || 100 }}
          JS_ISSUES=${{ steps.js-analysis.outputs.js_issues || 0 }}
          JS_SCORE=${{ steps.js-analysis.outputs.js_quality_score || 100 }}
          
          # Calculate weighted overall score (Python 70%, JS 30%)
          OVERALL_SCORE=$(echo "scale=1; ($PYTHON_SCORE * 0.7) + ($JS_SCORE * 0.3)" | bc -l)
          TOTAL_ISSUES=$((PYTHON_ISSUES + JS_ISSUES))
          
          echo "Overall quality score: $OVERALL_SCORE"
          echo "Total issues: $TOTAL_ISSUES"
          
          # Define thresholds based on gate level
          case "${{ env.GATE_LEVEL }}" in
            "basic")
              MIN_SCORE=60
              MAX_ISSUES=50
              ;;
            "standard")
              MIN_SCORE=75
              MAX_ISSUES=25
              ;;
            "strict")
              MIN_SCORE=85
              MAX_ISSUES=15
              ;;
            "enterprise")
              MIN_SCORE=90
              MAX_ISSUES=10
              ;;
          esac
          
          # Determine gate status
          if (( $(echo "$OVERALL_SCORE >= $MIN_SCORE" | bc -l) )) && (( TOTAL_ISSUES <= MAX_ISSUES )); then
            GATE_STATUS="passed"
            echo "âœ… Code Quality Gate: PASSED"
          else
            GATE_STATUS="failed"
            echo "âŒ Code Quality Gate: FAILED"
          fi
          
          echo "quality_score=$OVERALL_SCORE" >> $GITHUB_OUTPUT
          echo "issues_count=$TOTAL_ISSUES" >> $GITHUB_OUTPUT
          echo "gate_status=$GATE_STATUS" >> $GITHUB_OUTPUT

      - name: ðŸ“Š Upload Quality Reports
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-reports
          path: |
            **/*-report.json
            **/*-report.txt
          retention-days: 30

  # ðŸ§ª Gate 2: Test Coverage & Quality
  test-coverage-gate:
    name: ðŸ§ª Gate 2 - Test Coverage
    runs-on: ubuntu-latest
    timeout-minutes: 20
    outputs:
      coverage_percentage: ${{ steps.coverage.outputs.coverage_percentage }}
      test_results: ${{ steps.coverage.outputs.test_results }}
      gate_status: ${{ steps.coverage.outputs.gate_status }}
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: ðŸ”§ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-test.txt
          pip install pytest-cov pytest-html coverage

      - name: ðŸ§ª Run Tests with Coverage
        id: test-execution
        run: |
          echo "ðŸ§ª Running comprehensive test suite with coverage..."
          
          # Run tests with coverage
          pytest \
            --cov=src \
            --cov=backend \
            --cov-report=xml:coverage.xml \
            --cov-report=html:htmlcov \
            --cov-report=term-missing \
            --cov-fail-under=${{ env.COVERAGE_THRESHOLD }} \
            --junitxml=test-results.xml \
            --html=test-report.html \
            --self-contained-html \
            -v
          
          PYTEST_EXIT_CODE=$?
          echo "pytest_exit_code=$PYTEST_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: ðŸ“Š Test Coverage Analysis
        id: coverage
        run: |
          echo "ðŸ“Š Analyzing test coverage..."
          
          # Extract coverage percentage
          COVERAGE_PERCENT=$(coverage report | grep TOTAL | awk '{print $4}' | sed 's/%//')
          echo "Coverage percentage: $COVERAGE_PERCENT%"
          
          # Count test results
          if [ -f test-results.xml ]; then
            TESTS_TOTAL=$(grep -o 'tests="[0-9]*"' test-results.xml | sed 's/tests="//;s/"//' || echo 0)
            TESTS_FAILED=$(grep -o 'failures="[0-9]*"' test-results.xml | sed 's/failures="//;s/"//' || echo 0)
            TESTS_ERRORS=$(grep -o 'errors="[0-9]*"' test-results.xml | sed 's/errors="//;s/"//' || echo 0)
            TESTS_PASSED=$((TESTS_TOTAL - TESTS_FAILED - TESTS_ERRORS))
          else
            TESTS_TOTAL=0
            TESTS_PASSED=0
            TESTS_FAILED=0
            TESTS_ERRORS=0
          fi
          
          echo "Test results: $TESTS_PASSED passed, $TESTS_FAILED failed, $TESTS_ERRORS errors"
          
          # Test coverage gate decision
          PYTEST_RESULT=${{ steps.test-execution.outputs.pytest_exit_code }}
          COVERAGE_THRESHOLD=${{ env.COVERAGE_THRESHOLD }}
          
          if [[ $PYTEST_RESULT -eq 0 ]] && (( $(echo "$COVERAGE_PERCENT >= $COVERAGE_THRESHOLD" | bc -l) )); then
            GATE_STATUS="passed"
            echo "âœ… Test Coverage Gate: PASSED"
          else
            GATE_STATUS="failed"
            echo "âŒ Test Coverage Gate: FAILED"
          fi
          
          # Test quality assessment
          if [[ $TESTS_TOTAL -gt 0 ]]; then
            TEST_PASS_RATE=$(echo "scale=1; ($TESTS_PASSED * 100) / $TESTS_TOTAL" | bc -l)
          else
            TEST_PASS_RATE=0
          fi
          
          echo "coverage_percentage=$COVERAGE_PERCENT" >> $GITHUB_OUTPUT
          echo "test_results=$TESTS_PASSED/$TESTS_TOTAL (${TEST_PASS_RATE}% pass rate)" >> $GITHUB_OUTPUT
          echo "gate_status=$GATE_STATUS" >> $GITHUB_OUTPUT

      - name: ðŸ“Š Upload Coverage Reports
        uses: actions/upload-artifact@v4
        with:
          name: test-coverage-reports
          path: |
            coverage.xml
            htmlcov/
            test-results.xml
            test-report.html
          retention-days: 30

  # ðŸš€ Gate 3: Performance & Security
  performance-security-gate:
    name: ðŸš€ Gate 3 - Performance & Security
    runs-on: ubuntu-latest
    timeout-minutes: 25
    outputs:
      performance_score: ${{ steps.perf-analysis.outputs.performance_score }}
      security_score: ${{ steps.sec-analysis.outputs.security_score }}
      gate_status: ${{ steps.gate-decision.outputs.gate_status }}
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: ðŸ”§ Install Tools
        run: |
          pip install -r requirements.txt
          pip install pytest-benchmark bandit safety pip-audit

      - name: ðŸ“Š Performance Benchmarks
        id: performance
        run: |
          echo "ðŸ“Š Running performance benchmarks..."
          
          # Run performance tests if they exist
          if [ -f scripts/simple_benchmark.py ]; then
            python scripts/simple_benchmark.py --json-output > performance-results.json
            
            # Extract performance metrics
            AVG_RESPONSE_TIME=$(jq '.avg_response_time_ms // 0' performance-results.json)
            THROUGHPUT=$(jq '.throughput_rps // 0' performance-results.json)
            
            echo "Average response time: ${AVG_RESPONSE_TIME}ms"
            echo "Throughput: ${THROUGHPUT} RPS"
            
            # Performance scoring (lower response time = better)
            if (( $(echo "$AVG_RESPONSE_TIME <= 100" | bc -l) )); then
              PERF_SCORE=100
            elif (( $(echo "$AVG_RESPONSE_TIME <= 300" | bc -l) )); then
              PERF_SCORE=85
            elif (( $(echo "$AVG_RESPONSE_TIME <= 1000" | bc -l) )); then
              PERF_SCORE=70
            else
              PERF_SCORE=50
            fi
          else
            echo "No performance benchmarks found, using default score"
            PERF_SCORE=80
          fi
          
          echo "performance_score=$PERF_SCORE" >> $GITHUB_OUTPUT

      - name: ðŸ”’ Security Analysis
        id: security
        run: |
          echo "ðŸ”’ Running security analysis..."
          
          SECURITY_ISSUES=0
          
          # Bandit security scan
          if bandit -r src backend -f json -o bandit-results.json; then
            BANDIT_ISSUES=0
          else
            BANDIT_ISSUES=$(jq '.results | length' bandit-results.json || echo 0)
          fi
          echo "Bandit security issues: $BANDIT_ISSUES"
          
          # Safety vulnerability scan
          if safety check --json --output safety-results.json; then
            SAFETY_ISSUES=0
          else
            SAFETY_ISSUES=$(jq '.vulnerabilities | length' safety-results.json || echo 0)
          fi
          echo "Safety vulnerabilities: $SAFETY_ISSUES"
          
          # pip-audit scan
          if pip-audit --requirement requirements.txt --format json --output pip-audit-results.json; then
            PIP_AUDIT_ISSUES=0
          else
            PIP_AUDIT_ISSUES=$(jq '.vulnerabilities | length' pip-audit-results.json || echo 0)
          fi
          echo "pip-audit vulnerabilities: $PIP_AUDIT_ISSUES"
          
          TOTAL_SECURITY_ISSUES=$((BANDIT_ISSUES + SAFETY_ISSUES + PIP_AUDIT_ISSUES))
          
          # Security scoring
          if [[ $TOTAL_SECURITY_ISSUES -eq 0 ]]; then
            SEC_SCORE=100
          elif [[ $TOTAL_SECURITY_ISSUES -le 2 ]]; then
            SEC_SCORE=85
          elif [[ $TOTAL_SECURITY_ISSUES -le 5 ]]; then
            SEC_SCORE=70
          else
            SEC_SCORE=40
          fi
          
          echo "Total security issues: $TOTAL_SECURITY_ISSUES"
          echo "security_score=$SEC_SCORE" >> $GITHUB_OUTPUT
          echo "security_issues=$TOTAL_SECURITY_ISSUES" >> $GITHUB_OUTPUT

      - name: ðŸŽ¯ Performance & Security Gate Decision
        id: gate-decision
        run: |
          echo "ðŸŽ¯ Evaluating performance & security gate..."
          
          PERF_SCORE=${{ steps.performance.outputs.performance_score }}
          SEC_SCORE=${{ steps.security.outputs.security_score }}
          
          # Combined score (Performance 40%, Security 60%)
          COMBINED_SCORE=$(echo "scale=1; ($PERF_SCORE * 0.4) + ($SEC_SCORE * 0.6)" | bc -l)
          
          echo "Performance score: $PERF_SCORE"
          echo "Security score: $SEC_SCORE"
          echo "Combined score: $COMBINED_SCORE"
          
          # Gate thresholds based on level
          case "${{ env.GATE_LEVEL }}" in
            "basic")
              MIN_COMBINED_SCORE=60
              ;;
            "standard")
              MIN_COMBINED_SCORE=70
              ;;
            "strict")
              MIN_COMBINED_SCORE=80
              ;;
            "enterprise")
              MIN_COMBINED_SCORE=85
              ;;
          esac
          
          if (( $(echo "$COMBINED_SCORE >= $MIN_COMBINED_SCORE" | bc -l) )); then
            GATE_STATUS="passed"
            echo "âœ… Performance & Security Gate: PASSED"
          else
            GATE_STATUS="failed"
            echo "âŒ Performance & Security Gate: FAILED"
          fi
          
          echo "gate_status=$GATE_STATUS" >> $GITHUB_OUTPUT
          echo "combined_score=$COMBINED_SCORE" >> $GITHUB_OUTPUT

      - name: ðŸ“Š Upload Performance & Security Reports
        uses: actions/upload-artifact@v4
        with:
          name: performance-security-reports
          path: |
            performance-results.json
            bandit-results.json
            safety-results.json
            pip-audit-results.json
          retention-days: 30

  # ðŸ“‹ Gate 4: Compliance & Documentation
  compliance-gate:
    name: ðŸ“‹ Gate 4 - Compliance
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: contains(fromJSON('["strict", "enterprise"]'), env.GATE_LEVEL)
    outputs:
      compliance_score: ${{ steps.compliance.outputs.compliance_score }}
      gate_status: ${{ steps.compliance.outputs.gate_status }}
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ðŸ“‹ Compliance Checks
        id: compliance
        run: |
          echo "ðŸ“‹ Running compliance validation..."
          
          COMPLIANCE_SCORE=100
          COMPLIANCE_ISSUES=0
          
          # Check for required documentation
          if [[ ! -f README.md ]]; then
            echo "âŒ Missing README.md"
            COMPLIANCE_ISSUES=$((COMPLIANCE_ISSUES + 1))
          fi
          
          if [[ ! -f CHANGELOG.md ]] && [[ ! -f CHANGES.md ]]; then
            echo "âš ï¸ Missing CHANGELOG.md"
            COMPLIANCE_ISSUES=$((COMPLIANCE_ISSUES + 1))
          fi
          
          # Check for license
          if [[ ! -f LICENSE ]] && [[ ! -f LICENSE.txt ]] && [[ ! -f LICENSE.md ]]; then
            echo "âš ï¸ Missing LICENSE file"
            COMPLIANCE_ISSUES=$((COMPLIANCE_ISSUES + 1))
          fi
          
          # Check for security policy
          if [[ ! -f SECURITY.md ]] && [[ ! -f .github/SECURITY.md ]]; then
            echo "âš ï¸ Missing SECURITY.md"
            COMPLIANCE_ISSUES=$((COMPLIANCE_ISSUES + 1))
          fi
          
          # Check for contribution guidelines
          if [[ ! -f CONTRIBUTING.md ]] && [[ ! -f .github/CONTRIBUTING.md ]]; then
            echo "âš ï¸ Missing CONTRIBUTING.md"
            COMPLIANCE_ISSUES=$((COMPLIANCE_ISSUES + 1))
          fi
          
          # Check for code of conduct
          if [[ ! -f CODE_OF_CONDUCT.md ]] && [[ ! -f .github/CODE_OF_CONDUCT.md ]]; then
            echo "âš ï¸ Missing CODE_OF_CONDUCT.md"
            COMPLIANCE_ISSUES=$((COMPLIANCE_ISSUES + 1))
          fi
          
          # Check for API documentation
          if [[ ! -f API_ENDPOINTS.md ]] && [[ ! -d docs/api ]]; then
            echo "âš ï¸ Missing API documentation"
            COMPLIANCE_ISSUES=$((COMPLIANCE_ISSUES + 1))
          fi
          
          # Calculate compliance score
          COMPLIANCE_SCORE=$((100 - (COMPLIANCE_ISSUES * 10)))
          if [[ $COMPLIANCE_SCORE -lt 0 ]]; then
            COMPLIANCE_SCORE=0
          fi
          
          echo "Compliance issues: $COMPLIANCE_ISSUES"
          echo "Compliance score: $COMPLIANCE_SCORE"
          
          # Gate decision
          MIN_COMPLIANCE_SCORE=70
          if [[ "${{ env.GATE_LEVEL }}" == "enterprise" ]]; then
            MIN_COMPLIANCE_SCORE=85
          fi
          
          if [[ $COMPLIANCE_SCORE -ge $MIN_COMPLIANCE_SCORE ]]; then
            GATE_STATUS="passed"
            echo "âœ… Compliance Gate: PASSED"
          else
            GATE_STATUS="failed"
            echo "âŒ Compliance Gate: FAILED"
          fi
          
          echo "compliance_score=$COMPLIANCE_SCORE" >> $GITHUB_OUTPUT
          echo "gate_status=$GATE_STATUS" >> $GITHUB_OUTPUT

  # ðŸ† Final Quality Score & Gate Decision
  quality-score:
    name: ðŸ† Quality Score & Final Gate
    needs: [code-quality-gate, test-coverage-gate, performance-security-gate, compliance-gate]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      quality_score: ${{ steps.final-score.outputs.quality_score }}
      gate_status: ${{ steps.final-score.outputs.gate_status }}
    steps:
      - name: ðŸ“Š Calculate Final Quality Score
        id: final-score
        run: |
          echo "ðŸ“Š Calculating final quality score..."
          
          # Get individual gate scores
          CODE_QUALITY_SCORE=${{ needs.code-quality-gate.outputs.quality_score || 0 }}
          COVERAGE_PERCENT=${{ needs.test-coverage-gate.outputs.coverage_percentage || 0 }}
          PERF_SEC_SCORE=${{ needs.performance-security-gate.outputs.performance_score || 80 }}
          COMPLIANCE_SCORE=${{ needs.compliance-gate.outputs.compliance_score || 100 }}
          
          # Get gate statuses
          CODE_QUALITY_STATUS="${{ needs.code-quality-gate.outputs.gate_status || 'skipped' }}"
          TEST_COVERAGE_STATUS="${{ needs.test-coverage-gate.outputs.gate_status || 'skipped' }}"
          PERF_SEC_STATUS="${{ needs.performance-security-gate.outputs.gate_status || 'skipped' }}"
          COMPLIANCE_STATUS="${{ needs.compliance-gate.outputs.gate_status || 'skipped' }}"
          
          echo "Gate Results:"
          echo "  Code Quality: $CODE_QUALITY_SCORE ($CODE_QUALITY_STATUS)"
          echo "  Test Coverage: $COVERAGE_PERCENT% ($TEST_COVERAGE_STATUS)"
          echo "  Performance & Security: $PERF_SEC_SCORE ($PERF_SEC_STATUS)"
          echo "  Compliance: $COMPLIANCE_SCORE ($COMPLIANCE_STATUS)"
          
          # Calculate weighted overall score
          OVERALL_SCORE=$(echo "scale=1; ($CODE_QUALITY_SCORE * 0.3) + ($COVERAGE_PERCENT * 0.25) + ($PERF_SEC_SCORE * 0.3) + ($COMPLIANCE_SCORE * 0.15)" | bc -l)
          
          echo "Overall Quality Score: $OVERALL_SCORE"
          
          # Final gate decision - ALL gates must pass
          FAILED_GATES=0
          
          [[ "$CODE_QUALITY_STATUS" == "failed" ]] && FAILED_GATES=$((FAILED_GATES + 1))
          [[ "$TEST_COVERAGE_STATUS" == "failed" ]] && FAILED_GATES=$((FAILED_GATES + 1))
          [[ "$PERF_SEC_STATUS" == "failed" ]] && FAILED_GATES=$((FAILED_GATES + 1))
          [[ "$COMPLIANCE_STATUS" == "failed" ]] && FAILED_GATES=$((FAILED_GATES + 1))
          
          # Overall threshold based on gate level
          case "${{ env.GATE_LEVEL }}" in
            "basic")
              MIN_OVERALL_SCORE=65
              ;;
            "standard")
              MIN_OVERALL_SCORE=75
              ;;
            "strict")
              MIN_OVERALL_SCORE=85
              ;;
            "enterprise")
              MIN_OVERALL_SCORE=90
              ;;
          esac
          
          if [[ $FAILED_GATES -eq 0 ]] && (( $(echo "$OVERALL_SCORE >= $MIN_OVERALL_SCORE" | bc -l) )); then
            FINAL_GATE_STATUS="passed"
            echo "ðŸŽ‰ QUALITY GATE: PASSED"
          else
            FINAL_GATE_STATUS="failed"
            echo "âŒ QUALITY GATE: FAILED"
            echo "   Failed gates: $FAILED_GATES"
            echo "   Score requirement: $MIN_OVERALL_SCORE (actual: $OVERALL_SCORE)"
          fi
          
          echo "quality_score=$OVERALL_SCORE" >> $GITHUB_OUTPUT
          echo "gate_status=$FINAL_GATE_STATUS" >> $GITHUB_OUTPUT

      - name: ðŸ“‹ Generate Quality Gate Summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          # ðŸŽ¯ Multi-Stage Quality Gate Results
          
          ## ðŸ“Š Overall Results
          
          **Quality Score**: ${{ steps.final-score.outputs.quality_score }}/100
          **Gate Status**: ${{ steps.final-score.outputs.gate_status == 'passed' && 'âœ… PASSED' || 'âŒ FAILED' }}
          **Gate Level**: ${{ env.GATE_LEVEL }}
          
          ## ðŸšª Individual Gates
          
          | Gate | Score | Status | Weight |
          |------|-------|--------|---------|
          | ðŸ” Code Quality | ${{ needs.code-quality-gate.outputs.quality_score || 'N/A' }} | ${{ needs.code-quality-gate.outputs.gate_status || 'skipped' }} | 30% |
          | ðŸ§ª Test Coverage | ${{ needs.test-coverage-gate.outputs.coverage_percentage || 'N/A' }}% | ${{ needs.test-coverage-gate.outputs.gate_status || 'skipped' }} | 25% |
          | ðŸš€ Performance & Security | ${{ needs.performance-security-gate.outputs.performance_score || 'N/A' }} | ${{ needs.performance-security-gate.outputs.gate_status || 'skipped' }} | 30% |
          | ðŸ“‹ Compliance | ${{ needs.compliance-gate.outputs.compliance_score || 'N/A' }} | ${{ needs.compliance-gate.outputs.gate_status || 'skipped' }} | 15% |
          
          ## ðŸ“ˆ Quality Metrics
          
          - **Code Issues**: ${{ needs.code-quality-gate.outputs.issues_count || 'N/A' }}
          - **Test Results**: ${{ needs.test-coverage-gate.outputs.test_results || 'N/A' }}
          - **Coverage Threshold**: ${{ env.COVERAGE_THRESHOLD }}%
          - **Gate Level**: ${{ env.GATE_LEVEL }}
          
          ## ðŸŽ¯ Quality Standards
          
          Each gate level has different thresholds:
          - **Basic**: Overall 65+, relaxed individual thresholds
          - **Standard**: Overall 75+, moderate individual thresholds  
          - **Strict**: Overall 85+, strict individual thresholds
          - **Enterprise**: Overall 90+, enterprise-grade thresholds
          
          EOF

      - name: âŒ Fail Build on Quality Gate Failure
        if: steps.final-score.outputs.gate_status == 'failed'
        run: |
          echo "âŒ Quality gate failed with score ${{ steps.final-score.outputs.quality_score }}"
          echo "ðŸŽ¯ Gate level: ${{ env.GATE_LEVEL }}"
          echo "ðŸ“Š Review the quality metrics above and address the issues"
          exit 1